{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaede-san0910/workshop-2022/blob/main/internship_2022_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# インターンシップ2022夏秋 ワークショップ\n",
        "アプリケーションの開発業務を体感する、ソフトウェア開発コース"
      ],
      "metadata": {
        "id": "TsP0o8tMdr5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pythonの超基本"
      ],
      "metadata": {
        "id": "ru0e-slOd4vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0-1.まず、雑に動かします。"
      ],
      "metadata": {
        "id": "-zBbe-Czd_Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 1\n",
        "b = 2\n",
        "\n",
        "print(a+b)"
      ],
      "metadata": {
        "id": "WBM_1R00dpZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0-2.標準入力を使う"
      ],
      "metadata": {
        "id": "liOPi7naeFzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = int(input(\"a = \"))\n",
        "b = int(input(\"b = \"))\n",
        "\n",
        "print(\"{} + {} = {}\".format(a, b, a+b))"
      ],
      "metadata": {
        "id": "8dRQysBodpVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colabでカメラを使ってみる"
      ],
      "metadata": {
        "id": "ixcpJmQlfnGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "import PIL\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "FMUQ1L8Lfo-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下記のコードを実行してみてください。  \n",
        "\"capture\"でカメラから静止画が取得できます。"
      ],
      "metadata": {
        "id": "ZCG7-BG2ghk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "print('Saved to {}'.format(filename))\n",
        "  \n",
        "# Show the image which was just taken.\n",
        "display(Image(filename))"
      ],
      "metadata": {
        "id": "mFaU3CPcf0FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 今回のお題、MediaPipeを使ってみる\n",
        "\n",
        "GoogleのオープンソースソフトウェアであるMediaPipeを使ってみます。  \n",
        "プロジェクトページ  \n",
        "https://google.github.io/mediapipe/  \n",
        "Github  \n",
        "https://github.com/google/mediapipe  "
      ],
      "metadata": {
        "id": "PrSlsjd1eZ13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. まず、ライセンスを確認する\n",
        "オープンソースソフトウェアを使用する上で、初めに、ライセンスは確認するようにしてください。  \n",
        "オープンソースソフトウェアは、ライセンス条項に従うことを条件に、自由に使って良いこととなっております。  \n",
        "研究利用や商用利用が可能なものも多くあります。  \n",
        "  \n",
        "Google社は、下記のライセンスをもとに、MediaPipeの利用を認めております。  "
      ],
      "metadata": {
        "id": "12_EYoXheqG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apache-2.0 License\n",
        "比較的利用条件の緩いライセンスで、商用利用可、改変・複製可、公開・再配布も可などが認められます。  \n",
        "ただし、再配布する場合は、同じApache-2.0 Licenseで配布することが求められます。  \n",
        "ライセンス条件は、下記を確認ください。  \n",
        "https://www.apache.org/licenses/LICENSE-2.0"
      ],
      "metadata": {
        "id": "ST_IQeeNe6JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. mediapipeをインストールをする。\n",
        "python3がインストールされている状態であれば、次のコマンドでインストールは完了です。"
      ],
      "metadata": {
        "id": "hSeIIxX0fKL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "BzTJU6IKdpPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考：https://google.github.io/mediapipe/getting_started/python.html"
      ],
      "metadata": {
        "id": "9PtOI9wHfV9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. サンプルコードを動かしてみる\n",
        "オープンソースソフトウェアは、基本的に、サンプルのソースコードも公開されております。  \n",
        "ハンドトラッキングの機能を動かしてみます。  \n",
        "https://google.github.io/mediapipe/solutions/hands"
      ],
      "metadata": {
        "id": "EN59PnPnfenE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# 画像を撮影する ←ココ追加！\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [filename]    #  ←ココ追加！撮影した画像を解析対象とする。\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Print handedness and draw hand landmarks on the image.\n",
        "    print('Handedness:', results.multi_handedness)\n",
        "    if not results.multi_hand_landmarks:\n",
        "      continue\n",
        "    image_height, image_width, _ = image.shape\n",
        "    annotated_image = image.copy()\n",
        "    for hand_landmarks in results.multi_hand_landmarks:\n",
        "      print('hand_landmarks:', hand_landmarks)\n",
        "      print(\n",
        "          f'Index finger tip coordinates: (',\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
        "      )\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image,\n",
        "          hand_landmarks,\n",
        "          mp_hands.HAND_CONNECTIONS,\n",
        "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "          mp_drawing_styles.get_default_hand_connections_style())\n",
        "    cv2.imwrite(\n",
        "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "    # Draw hand world landmarks.\n",
        "    if not results.multi_hand_world_landmarks:\n",
        "      continue\n",
        "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "      mp_drawing.plot_landmarks(\n",
        "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
      ],
      "metadata": {
        "id": "TfH3BoDndpMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image('/tmp/annotated_image' + str(idx) + '.png'))"
      ],
      "metadata": {
        "id": "B5pf8yjflI5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. 任意の手の座標情報を出力してみる。\n",
        "mediapipeは、２つの座標系での座標情報を出力することが可能です。  \n",
        "\n",
        "#### MULTI_HAND_LANDMARKS  \n",
        "Collection of detected/tracked hands, where each hand is represented as a list of 21 hand landmarks and each landmark is composed of x, y and z. x and y are normalized to [0.0, 1.0] by the image width and height respectively. z represents the landmark depth with the depth at the wrist being the origin, and the smaller the value the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x.\n",
        "  \n",
        "#### MULTI_HAND_WORLD_LANDMARKS  \n",
        "Collection of detected/tracked hands, where each hand is represented as a list of 21 hand landmarks in world coordinates. Each landmark is composed of x, y and z: real-world 3D coordinates in meters with the origin at the hand’s approximate geometric center.\n",
        "  \n",
        "それぞれの座標系における人差し指の座標情報を出力してみましょう。  \n",
        "\n"
      ],
      "metadata": {
        "id": "QVY4lQ3QjLz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MULTI_HAND_LANDMARKS"
      ],
      "metadata": {
        "id": "9_6W9gcWmCK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# 画像を撮影する ←ココ追加！\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [filename]    #  ←ココ追加！撮影した画像を解析対象とする。\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Print hand world landmarks.\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        print(\n",
        "            f'x={hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x}, '\n",
        "            f'y={hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y}, '\n",
        "            f'z={hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z}'\n",
        "        )\n",
        "\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imwrite(\n",
        "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "\n",
        "    # 画像を表示\n",
        "    display(Image('/tmp/annotated_image' + str(idx) + '.png'))"
      ],
      "metadata": {
        "id": "wlZOwg_tdpJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MULTI_HAND_WORLD_LANDMARKS"
      ],
      "metadata": {
        "id": "NICd7yNsmJz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# 画像を撮影する ←ココ追加！\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [filename]    #  ←ココ追加！撮影した画像を解析対象とする。\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Print hand world landmarks.\n",
        "    if results.multi_hand_world_landmarks:\n",
        "      for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "        print(\n",
        "            f'x={hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x}, '\n",
        "            f'y={hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y}, '\n",
        "            f'z={hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z}'\n",
        "        )\n",
        "\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imwrite(\n",
        "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "\n",
        "    # 画像を表示\n",
        "    display(Image('/tmp/annotated_image' + str(idx) + '.png'))"
      ],
      "metadata": {
        "id": "XyOKC7aIdo_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "２つの座標系の使い分けはイメージできますか？  \n",
        "例えば、ジェスチャー認識をさせる場合は、ワールド座標系を用いた方が良さそうです。"
      ],
      "metadata": {
        "id": "zugIBuxRnqpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-5. ジェスチャー認識をさせてみましょう。\n",
        "先ほど取得した人差し指座標を用いて、０と１を認識させてみましょう。  \n",
        "0と１の違いは、どんなパラメータを用いて区別することができるでしょうか？"
      ],
      "metadata": {
        "id": "iL_etGQGovVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0 で撮影してみる"
      ],
      "metadata": {
        "id": "xRc9jkn-pDmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# 画像を撮影する ←ココ追加！\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [filename]    #  ←ココ追加！撮影した画像を解析対象とする。\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Print hand world landmarks.\n",
        "    if results.multi_hand_world_landmarks:\n",
        "      for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "\n",
        "        x = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
        "        y = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
        "        z = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z\n",
        "        dist = (x**2 + y**2 + z**2)**(0.5)\n",
        "\n",
        "        print(\n",
        "            f'x={x}, '\n",
        "            f'y={y}, '\n",
        "            f'z={z}, '\n",
        "            f'd={dist}'\n",
        "        )\n",
        "\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imwrite(\n",
        "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "\n",
        "    # 画像を表示\n",
        "    display(Image('/tmp/annotated_image' + str(idx) + '.png'))"
      ],
      "metadata": {
        "id": "OE8ltDAFdo8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1で撮影してみる"
      ],
      "metadata": {
        "id": "jPAfBd97pL4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# 画像を撮影する ←ココ追加！\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [filename]    #  ←ココ追加！撮影した画像を解析対象とする。\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Print hand world landmarks.\n",
        "    if results.multi_hand_world_landmarks:\n",
        "      for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "\n",
        "        x = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
        "        y = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
        "        z = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z\n",
        "        dist = (x**2 + y**2 + z**2)**(0.5)\n",
        "\n",
        "        print(\n",
        "            f'x={x}, '\n",
        "            f'y={y}, '\n",
        "            f'z={z}, '\n",
        "            f'd={dist}'\n",
        "        )\n",
        "\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imwrite(\n",
        "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "\n",
        "    # 画像を表示\n",
        "    display(Image('/tmp/annotated_image' + str(idx) + '.png'))"
      ],
      "metadata": {
        "id": "ael7rdP9do5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "人差し指とワールド座標系の中心(0, 0, 0)の距離に着目すれば、  \n",
        "１と０の区別はできそうだとわかりました。  \n",
        "閾値を決めてジェスチャーを出力してみましょう！"
      ],
      "metadata": {
        "id": "NKx2AlncqLS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# 画像を撮影する ←ココ追加！\n",
        "filename = take_photo(filename='photo.jpg')\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [filename]    #  ←ココ追加！撮影した画像を解析対象とする。\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Print hand world landmarks.\n",
        "    if results.multi_hand_world_landmarks:\n",
        "      for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "\n",
        "        x = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
        "        y = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
        "        z = hand_world_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z\n",
        "        dist = (x**2 + y**2 + z**2)**(0.5)\n",
        "\n",
        "        gesture = \"1\" if dist > 0.06 else 0\n",
        "         \n",
        "        print(\n",
        "            f'gesuture={gesture}, '\n",
        "            f'dist={dist}'\n",
        "        )\n",
        "\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imwrite(\n",
        "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "\n",
        "    # 画像を表示\n",
        "    display(Image('/tmp/annotated_image' + str(idx) + '.png'))"
      ],
      "metadata": {
        "id": "dOXk-x7MqLuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-6. ハンドトラッキングの理論を確認してみましょう。\n",
        "下記に本件に関する文献のURLを貼っておきます。  \n",
        "興味があれば、確認してみてください！  \n",
        "https://arxiv.org/abs/2006.10214  \n",
        "https://www.youtube.com/watch?v=I-UOrvxxXEk"
      ],
      "metadata": {
        "id": "wVGaro-TqiCd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ksh0Pn9Cqq1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}